{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import typing as t\n",
    "import vizdoom\n",
    "\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import evaluation, policies\n",
    "from torch import nn\n",
    "\n",
    "#from common import envs, plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "# 1 per kill\n",
    "reward_factor_frag = 1.0\n",
    "reward_factor_damage = 0.01\n",
    "\n",
    "# Player can move at ~16.66 units per tick\n",
    "reward_factor_distance = 5e-4\n",
    "penalty_factor_distance = -2.5e-3\n",
    "reward_threshold_distance = 3.0\n",
    "\n",
    "# Pistol clips have 10 bullets\n",
    "reward_factor_ammo_increment = 0.02\n",
    "reward_factor_ammo_decrement = -0.01\n",
    "\n",
    "# Player starts at 100 health\n",
    "reward_factor_health_increment = 0.02\n",
    "reward_factor_health_decrement = -0.01\n",
    "reward_factor_armor_increment = 0.01"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment wrapper for VizDoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "from gym import spaces\n",
    "from vizdoom.vizdoom import GameVariable\n",
    "\n",
    "# List of game variables storing ammunition information. Used for keeping track of ammunition-related rewards.\n",
    "AMMO_VARIABLES = [GameVariable.AMMO0, GameVariable.AMMO1, GameVariable.AMMO2, GameVariable.AMMO3, GameVariable.AMMO4,\n",
    "                  GameVariable.AMMO5, GameVariable.AMMO6, GameVariable.AMMO7, GameVariable.AMMO8, GameVariable.AMMO9]\n",
    "\n",
    "# List of game variables storing weapon information. Used for keeping track of ammunition-related rewards.\n",
    "WEAPON_VARIABLES = [GameVariable.WEAPON0, GameVariable.WEAPON1, GameVariable.WEAPON2, GameVariable.WEAPON3,\n",
    "                    GameVariable.WEAPON4,\n",
    "                    GameVariable.WEAPON5, GameVariable.WEAPON6, GameVariable.WEAPON7, GameVariable.WEAPON8,\n",
    "                    GameVariable.WEAPON9]\n",
    "\n",
    "class DoomWithBotsShaped(envs.DoomWithBots):\n",
    "    \"\"\"An environment wrapper for a Doom deathmatch game with bots. \n",
    "    \n",
    "    Rewards are shaped according to the multipliers defined in the notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots, shaping):\n",
    "        super().__init__(game, frame_processor, frame_skip, n_bots)\n",
    "\n",
    "        # Give a random two-letter name to the agent for identifying instances in parallel learning.\n",
    "        self.name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=2))\n",
    "        self.shaping = shaping\n",
    "\n",
    "        # Internal states\n",
    "        self.last_health = 100\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.ammo_state = self._get_ammo_state()\n",
    "        self.weapon_state = self._get_weapon_state()\n",
    "        self.total_rew = self.last_damage_dealt = self.deaths = self.last_frags = self.last_armor = 0\n",
    "\n",
    "        # Store individual reward contributions for logging purposes\n",
    "        self.rewards_stats = {\n",
    "            'frag': 0,\n",
    "            'damage': 0,\n",
    "            'ammo': 0,\n",
    "            'health': 0,\n",
    "            'armor': 0,\n",
    "            'distance': 0,\n",
    "        }\n",
    "        \n",
    "    def step(self, action, array=False):\n",
    "        # Perform the action as usual\n",
    "        state, reward, done, info = super().step(action)\n",
    "        \n",
    "        self._log_reward_stat('frag', reward)\n",
    "\n",
    "        # Adjust the reward based on the shaping table\n",
    "        if self.shaping:\n",
    "            shaped_reward = reward + self.shape_rewards()\n",
    "        else:\n",
    "            shaped_reward = reward\n",
    "\n",
    "        self.total_rew += shaped_reward\n",
    "\n",
    "        return state, shaped_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._print_state()\n",
    "        \n",
    "        state = super().reset()\n",
    "\n",
    "        self.last_health = 100\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.last_armor = self.last_frags = self.total_rew = self.deaths = 0\n",
    "\n",
    "        # Damage count  is not cleared when starting a new episode: https://github.com/mwydmuch/ViZDoom/issues/399\n",
    "        # self.last_damage_dealt = 0\n",
    "\n",
    "        # Reset reward stats\n",
    "        for k in self.rewards_stats.keys():\n",
    "            self.rewards_stats[k] = 0\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def shape_rewards(self):\n",
    "        reward_contributions = [\n",
    "            self._compute_damage_reward(),\n",
    "            self._compute_ammo_reward(),\n",
    "            self._compute_health_reward(),\n",
    "            self._compute_armor_reward(),\n",
    "            self._compute_distance_reward(*self._get_player_pos()),\n",
    "        ]\n",
    "\n",
    "        return sum(reward_contributions)\n",
    "    \n",
    "    def _respawn_if_dead(self):\n",
    "        if not self.game.is_episode_finished():\n",
    "            # Check if player is dead\n",
    "            if self.game.is_player_dead():\n",
    "                self.deaths += 1\n",
    "                self._reset_player()\n",
    "\n",
    "    def _compute_distance_reward(self, x, y):\n",
    "        \"\"\"Computes a reward/penalty based on the distance travelled since last update.\"\"\"\n",
    "        dx = self.last_x - x\n",
    "        dy = self.last_y - y\n",
    "\n",
    "        distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "\n",
    "        if distance - reward_threshold_distance > 0:\n",
    "            reward = reward_factor_distance\n",
    "        else:\n",
    "            reward = -reward_factor_distance\n",
    "\n",
    "        self.last_x = x\n",
    "        self.last_y = y\n",
    "        self._log_reward_stat('distance', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_damage_reward(self):\n",
    "        \"\"\"Computes a reward based on total damage inflicted to enemies since last update.\"\"\"\n",
    "        damage_dealt = self.game.get_game_variable(GameVariable.DAMAGECOUNT)\n",
    "        reward = reward_factor_damage * (damage_dealt - self.last_damage_dealt)\n",
    "\n",
    "        self.last_damage_dealt = damage_dealt\n",
    "        self._log_reward_stat('damage', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_health_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total health change since last update.\"\"\"\n",
    "        # When the player is dead, the health game variable can be -999900\n",
    "        health = max(self.game.get_game_variable(GameVariable.HEALTH), 0)\n",
    "\n",
    "        health_reward = reward_factor_health_increment * max(0, health - self.last_health)\n",
    "        health_penalty = reward_factor_health_decrement * min(0, health - self.last_health)\n",
    "        reward = health_reward - health_penalty\n",
    "\n",
    "        self.last_health = health\n",
    "        self._log_reward_stat('health', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_armor_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total armor change since last update.\"\"\"\n",
    "        armor = self.game.get_game_variable(GameVariable.ARMOR)\n",
    "        reward = reward_factor_armor_increment * max(0, armor - self.last_armor)\n",
    "        \n",
    "        self.last_armor = armor\n",
    "        self._log_reward_stat('armor', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_ammo_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total ammunition change since last update.\"\"\"\n",
    "        self.weapon_state = self._get_weapon_state()\n",
    "\n",
    "        new_ammo_state = self._get_ammo_state()\n",
    "        ammo_diffs = (new_ammo_state - self.ammo_state) * self.weapon_state\n",
    "        ammo_reward = reward_factor_ammo_increment * max(0, np.sum(ammo_diffs))\n",
    "        ammo_penalty = reward_factor_ammo_decrement * min(0, np.sum(ammo_diffs))\n",
    "        reward = ammo_reward - ammo_penalty\n",
    "        \n",
    "        self.ammo_state = new_ammo_state\n",
    "        self._log_reward_stat('ammo', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _get_player_pos(self):\n",
    "        \"\"\"Returns the player X- and Y- coordinates.\"\"\"\n",
    "        return self.game.get_game_variable(GameVariable.POSITION_X), self.game.get_game_variable(\n",
    "            GameVariable.POSITION_Y)\n",
    "\n",
    "    def _get_ammo_state(self):\n",
    "        \"\"\"Returns the total available ammunition per weapon slot.\"\"\"\n",
    "        ammo_state = np.zeros(10)\n",
    "\n",
    "        for i in range(10):\n",
    "            ammo_state[i] = self.game.get_game_variable(AMMO_VARIABLES[i])\n",
    "\n",
    "        return ammo_state\n",
    "\n",
    "    def _get_weapon_state(self):\n",
    "        \"\"\"Returns which weapon slots can be used. Available weapons are encoded as ones.\"\"\"\n",
    "        weapon_state = np.zeros(10)\n",
    "\n",
    "        for i in range(10):\n",
    "            weapon_state[i] = self.game.get_game_variable(WEAPON_VARIABLES[i])\n",
    "\n",
    "        return weapon_state\n",
    "\n",
    "    def _log_reward_stat(self, kind: str, reward: float):\n",
    "        self.rewards_stats[kind] += reward\n",
    "\n",
    "    def _reset_player(self):\n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.game.respawn_player()\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.ammo_state = self._get_ammo_state()\n",
    "\n",
    "    def _print_state(self):\n",
    "        super()._print_state()\n",
    "        print('\\nREWARD BREAKDOWN')\n",
    "        print('Agent {} frags: {}, deaths: {}, total reward: {:.2f}'.format(\n",
    "            self.name,\n",
    "            self.last_frags,\n",
    "            self.deaths,\n",
    "            self.total_rew\n",
    "        ))\n",
    "        for k, v in self.rewards_stats.items():\n",
    "            print(f'- {k}: {v:+.1f}')\n",
    "        print('***************************************\\n\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv\n",
    "\n",
    "def game_instance(scenario):\n",
    "    \"\"\"Creates a Doom game instance.\"\"\"\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config(f'./scenarios/{scenario}.cfg')\n",
    "    game.add_game_args(envs.DOOM_ENV_WITH_BOTS_ARGS)\n",
    "    game.set_window_visible(False)\n",
    "    game.init()\n",
    "    \n",
    "    return game\n",
    "\n",
    "def env_with_bots_shaped(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    \"\"\"Wraps a Doom game instance in an environment with shaped rewards.\"\"\"\n",
    "    game = game_instance(scenario)\n",
    "    return DoomWithBotsShaped(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots_shaped(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    \"\"\"Wraps a Doom game instance in a vectorized environment with shaped rewards.\"\"\"\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots_shaped(**kwargs)] * n_envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.models import CustomCNN\n",
    "\n",
    "scenario = 'deathmatch_simple'\n",
    "\n",
    "# Agent parameters.\n",
    "agent_args = {\n",
    "    'n_epochs': 3,\n",
    "    'n_steps': 4096,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}\n",
    "\n",
    "# Environment parameters.\n",
    "env_args = {\n",
    "    'scenario': scenario,\n",
    "    'frame_skip': 4,\n",
    "    'frame_processor': envs.default_frame_processor,\n",
    "    'n_bots': 8,\n",
    "    'shaping': True\n",
    "}\n",
    "\n",
    "# In the evaluation environment we measure frags only.\n",
    "eval_env_args = dict(env_args)\n",
    "eval_env_args['shaping'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0, deaths: 0, total reward: 0.00\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +0.0\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.0\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0, deaths: 0, total reward: 0.00\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +0.0\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.0\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 4.55\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +4.0\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 4.95\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +4.4\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 3.28\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.8\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 3.24\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.7\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 2.15\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +1.6\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 2.79\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.3\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 3.29\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.7\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 3.75\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.2\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 2.64\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.1\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 2.95\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.4\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 3.18\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.6\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 3.73\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.2\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 3.08\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.5\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 2.03\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +1.5\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 4.00\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.4\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 4.09\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.5\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent EE frags: 0.0, deaths: 0, total reward: 2.94\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.4\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent YT frags: 0.0, deaths: 0, total reward: 3.76\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.2\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "SignalException",
     "evalue": "Signal SIGINT received. ViZDoom instance has been closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSignalException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m env \u001b[39m=\u001b[39m vec_env_with_bots_shaped(\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_args)\n\u001b[1;32m      3\u001b[0m eval_env \u001b[39m=\u001b[39m vec_env_with_bots_shaped(\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39meval_env_args)\n\u001b[0;32m----> 5\u001b[0m envs\u001b[39m.\u001b[39;49msolve_env(env, eval_env, scenario, agent_args)\n",
      "File \u001b[0;32m~/Desktop/DOOM_PPO/common/envs.py:210\u001b[0m, in \u001b[0;36msolve_env\u001b[0;34m(env, eval_env, scenario, agent_args)\u001b[0m\n\u001b[1;32m    202\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(\n\u001b[1;32m    203\u001b[0m     eval_env,\n\u001b[1;32m    204\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m    205\u001b[0m     eval_freq\u001b[39m=\u001b[39m\u001b[39m16384\u001b[39m,\n\u001b[1;32m    206\u001b[0m     log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/evaluations/\u001b[39m\u001b[39m{\u001b[39;00mscenario\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    207\u001b[0m     best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/models/\u001b[39m\u001b[39m{\u001b[39;00mscenario\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Start the training process.\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3000000\u001b[39;49m, tb_log_name\u001b[39m=\u001b[39;49mscenario, callback\u001b[39m=\u001b[39;49m[monitoring_callback, eval_callback])\n\u001b[1;32m    212\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    213\u001b[0m eval_env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m: PPOSelf,\n\u001b[1;32m    305\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PPOSelf:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    318\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    319\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    320\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    321\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    322\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    323\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    324\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    325\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    326\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    327\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 262\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    179\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 181\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    185\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mDoomWithBotsShaped.step\u001b[0;34m(self, action, array)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action, array\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     48\u001b[0m     \u001b[39m# Perform the action as usual\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     state, reward, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_reward_stat(\u001b[39m'\u001b[39m\u001b[39mfrag\u001b[39m\u001b[39m'\u001b[39m, reward)\n\u001b[1;32m     53\u001b[0m     \u001b[39m# Adjust the reward based on the shaping table\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DOOM_PPO/common/envs.py:113\u001b[0m, in \u001b[0;36mDoomWithBots.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mmake_action(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpossible_actions[action], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_skip)\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Compute rewards.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     frags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_game_variable(GameVariable\u001b[39m.\u001b[39mFRAGCOUNT)\n",
      "\u001b[0;31mSignalException\u001b[0m: Signal SIGINT received. ViZDoom instance has been closed."
     ]
    }
   ],
   "source": [
    "# Create environments with bots and shaping.\n",
    "env = vec_env_with_bots_shaped(2, **env_args)\n",
    "eval_env = vec_env_with_bots_shaped(1, **eval_env_args)\n",
    "\n",
    "envs.solve_env(env, eval_env, scenario, agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "REWARD_THRESHOLDS = [5, 10, 15, 20, 25, 25]\n",
    "\n",
    "class DoomWithBotsCurriculum(DoomWithBotsShaped):\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots, shaping, initial_level=0, max_level=5, rolling_mean_length=10):\n",
    "        super().__init__(game, frame_processor, frame_skip, n_bots, shaping)\n",
    "        \n",
    "        # Initialize ACS script difficulty level\n",
    "        game.send_game_command('pukename change_difficulty 0')\n",
    "        \n",
    "        # Internal state\n",
    "        self.level = initial_level\n",
    "        self.max_level = max_level\n",
    "        self.rolling_mean_length = rolling_mean_length\n",
    "        self.last_rewards = deque(maxlen=rolling_mean_length)\n",
    "\n",
    "    def step(self, action, array=False):\n",
    "        # Perform action step as usual\n",
    "        state, reward, done, infos = super().step(action, array)\n",
    "\n",
    "        # After an episode, check whether difficulty should be increased.\n",
    "        if done:\n",
    "            self.last_rewards.append(self.total_rew)\n",
    "            run_mean = np.mean(self.last_rewards)\n",
    "            print('Avg. last 10 runs of {}: {:.2f}. Current difficulty level: {}'.format(self.name, run_mean, self.level))\n",
    "            if run_mean > REWARD_THRESHOLDS[self.level] and len(self.last_rewards) >= self.rolling_mean_length:\n",
    "                self._change_difficulty()\n",
    "\n",
    "        return state, reward, done, infos\n",
    "\n",
    "    def reset(self):\n",
    "        state = super().reset()\n",
    "        self.game.send_game_command(f'pukename change_difficulty {self.level}')\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _change_difficulty(self):\n",
    "        \"\"\"Adjusts the difficulty by setting the difficulty level in the ACS script.\"\"\"\n",
    "        if self.level < self.max_level:\n",
    "            self.level += 1\n",
    "            print(f'Changing difficulty for {self.name} to {self.level}')\n",
    "            self.game.send_game_command(f'pukename change_difficulty {self.level}')\n",
    "            self.last_rewards = deque(maxlen=self.rolling_mean_length)\n",
    "        else:\n",
    "            print(f'{self.name} already at max level!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0, deaths: 0, total reward: 0.00\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +0.0\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.0\n",
      "***************************************\n",
      "\n",
      "\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0, deaths: 0, total reward: 0.00\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +0.0\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.0\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of 41: 4.73. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0.0, deaths: 0, total reward: 4.73\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +4.2\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of PJ: 5.17. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0.0, deaths: 0, total reward: 5.17\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +4.6\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of 41: 4.19. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0.0, deaths: 0, total reward: 3.64\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.1\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of PJ: 4.09. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0.0, deaths: 0, total reward: 3.00\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.5\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of 41: 3.81. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0.0, deaths: 0, total reward: 3.05\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.5\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of PJ: 3.89. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0.0, deaths: 0, total reward: 3.49\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.9\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of 41: 3.34. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0.0, deaths: 0, total reward: 1.95\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +1.4\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of PJ: 3.71. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0.0, deaths: 0, total reward: 3.19\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.6\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of 41: 3.34. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0.0, deaths: 0, total reward: 3.34\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.8\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.6\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of PJ: 3.11. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0.0, deaths: 0, total reward: 0.71\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +0.2\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of 41: 3.40. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent 41 frags: 0.0, deaths: 0, total reward: 3.69\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +3.2\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n",
      "Avg. last 10 runs of PJ: 3.08. Current difficulty level: 0\n",
      "*** DEATHMATCH RESULTS ***\n",
      " - AGENT: 0\n",
      "\n",
      "REWARD BREAKDOWN\n",
      "Agent PJ frags: 0.0, deaths: 0, total reward: 2.89\n",
      "- frag: +0.0\n",
      "- damage: +0.0\n",
      "- ammo: +2.4\n",
      "- health: +0.0\n",
      "- armor: +0.0\n",
      "- distance: +0.5\n",
      "***************************************\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m env \u001b[39m=\u001b[39m vec_env_with_bots_curriculum(\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_args)\n\u001b[1;32m     12\u001b[0m eval_env \u001b[39m=\u001b[39m vec_env_with_bots_shaped(\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39meval_env_args) \u001b[39m# Don't use adaptive curriculum for the evaluation env!\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m envs\u001b[39m.\u001b[39;49msolve_env(env, eval_env, scenario, agent_args)\n",
      "File \u001b[0;32m~/Desktop/DOOM_PPO/common/envs.py:210\u001b[0m, in \u001b[0;36msolve_env\u001b[0;34m(env, eval_env, scenario, agent_args)\u001b[0m\n\u001b[1;32m    202\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(\n\u001b[1;32m    203\u001b[0m     eval_env,\n\u001b[1;32m    204\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m    205\u001b[0m     eval_freq\u001b[39m=\u001b[39m\u001b[39m16384\u001b[39m,\n\u001b[1;32m    206\u001b[0m     log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/evaluations/\u001b[39m\u001b[39m{\u001b[39;00mscenario\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m    207\u001b[0m     best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/models/\u001b[39m\u001b[39m{\u001b[39;00mscenario\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[39m# Start the training process.\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3000000\u001b[39;49m, tb_log_name\u001b[39m=\u001b[39;49mscenario, callback\u001b[39m=\u001b[39;49m[monitoring_callback, eval_callback])\n\u001b[1;32m    212\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    213\u001b[0m eval_env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m: PPOSelf,\n\u001b[1;32m    305\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PPOSelf:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    318\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    319\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    320\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    321\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    322\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    323\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    324\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    325\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    326\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    327\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:283\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    285\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:276\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 276\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    277\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    278\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/vizdoom/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def env_with_bots_curriculum(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    \"\"\"Wraps a Doom game instance in an environment with shaped rewards and curriculum.\"\"\"\n",
    "    game = game_instance(scenario)\n",
    "    return DoomWithBotsCurriculum(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots_curriculum(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    \"\"\"Wraps a Doom game instance in a vectorized environment with shaped rewards and curriculum.\"\"\"\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots_curriculum(**kwargs)] * n_envs))\n",
    "\n",
    "# Create environments with bots.\n",
    "env = vec_env_with_bots_curriculum(2, **env_args)\n",
    "eval_env = vec_env_with_bots_shaped(1, **eval_env_args) # Don't use adaptive curriculum for the evaluation env!\n",
    "\n",
    "envs.solve_env(env, eval_env, scenario, agent_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vizdoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6749a172b7ea0e1d631e5a8659e6491c2bcc1d4c4ce575d079fb3bbcfc596a6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
